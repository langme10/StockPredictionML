{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91f5545-3b1c-493c-9e52-ea4d00989cde",
   "metadata": {},
   "source": [
    "!pip install yfinance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61164e2-add4-4cd7-9aec-f6f13a6ef9d5",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "##### Exploratory Data Analysis (EDA)\n",
    "- Check the structure of the dataset:\n",
    "    - Verify column names and data types\n",
    "- Identify missing values:\n",
    "    - Drop weekends as they are non trading days.\n",
    "    - If missing values exist in **Open/Close** and **High/Low**: Fill with the average of previous and next day's values. \n",
    "    - If missing values exist in **Volume**: Fill with the median.\n",
    "- Identify duplicate rows:\n",
    "  - If duplicate trading days exist for the same stock, drop them.\n",
    "\n",
    "##### Split dataset into training, test, and validation set\n",
    "- Use **70/15/15 split** for training, testing, and validation\n",
    "\n",
    "\n",
    "##### Feature Scaling \n",
    "- Scale features using **StandardScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b948075-e937-4853-bcfa-8f143b979d02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yfinance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yfinance'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6660e81-2d31-47d9-8a59-e202b2eeeb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data_files/stocks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stocks \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data_files/stocks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Display basic info and first few rows\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(stocks\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data_files/stocks.csv'"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "stocks = pd.read_csv(\"../data_files/stocks.csv\")\n",
    "\n",
    "# Display basic info and first few rows\n",
    "print(stocks.columns)\n",
    "stocks.info() \n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f3094d-543d-4eae-92fb-55a6518f7a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Types:\u001b[39m\u001b[38;5;124m\"\u001b[39m,stocks\u001b[38;5;241m.\u001b[39mdtypes )\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing Values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stocks\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stocks' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Data Types:\",stocks.dtypes )\n",
    "print(\"Missing Values:\", stocks.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fecb739-af46-49d8-93cd-85546a4282d9",
   "metadata": {},
   "source": [
    "Each stock has seperate columns for Close, High, Low, Open, and Volume. There are many NaN values in the dataset for stocks that did not exit or trade on a specific date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83415ed-4b8c-4642-9f53-6c45055d3874",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Deal with missing values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Stock market closed on weekends so drop weekends\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m stocks[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(stocks[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate_\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m stocks \u001b[38;5;241m=\u001b[39m stocks\u001b[38;5;241m.\u001b[39mloc[stocks[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate_\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofweek \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#https://gpttutorpro.com/pandas-dataframe-filtering-using-datetime-methods/\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Replace missing values in Open,Close, High, and Low columns\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stocks' is not defined"
     ]
    }
   ],
   "source": [
    "#Deal with missing values\n",
    "\n",
    "#Stock market closed on weekends so drop weekends\n",
    "stocks['Date_'] = pd.to_datetime(stocks['Date_'])\n",
    "stocks = stocks.loc[stocks['Date_'].dt.dayofweek < 5]\n",
    "#https://gpttutorpro.com/pandas-dataframe-filtering-using-datetime-methods/\n",
    "\n",
    "#Replace missing values in Open,Close, High, and Low columns\n",
    "for col in stocks.columns:\n",
    "    if \"Open\" in col or \"Close\" in col or \"High\" in col or \"Low\" in col:\n",
    "        # Find the first valid index where trading starts\n",
    "        first_valid_index = stocks[col].first_valid_index()\n",
    "\n",
    "        if first_valid_index is not None:\n",
    "            # Fill missing values only after the stock starts trading with average of previous and next day's values.\n",
    "            stocks.loc[first_valid_index:,col] = stocks.loc[first_valid_index:,col].fillna((stocks[col].shift(1)+stocks[col].shift(-1))/2)\n",
    "\n",
    "            # Forward-fill and backward-fill only after the first valid trading day (fills with closest)\n",
    "            stocks.loc[first_valid_index:,col] = stocks.loc[first_valid_index:,col].ffill().bfill()\n",
    "\n",
    "#https://medium.com/@farisyid/penggunaan-ffill-dan-bfill-pada-proses-data-cleaning-b4f3bfec9767#:~:text='ffill'%20which%20means%20forward%20fill%20and%20'bfill',such%20as%20DataFrame%20or%20Series%20in%20Pandas.&text=Instead%2C%20the%20'bfill'%20method%20fills%20the%20missing,the%20missing%20value%20in%20the%20data%20sequence.\n",
    "\n",
    "#replace missing values in volume columns with the median\n",
    "volume_cols = []\n",
    "for col in stocks.columns:\n",
    "    if \"Volume\" in col:\n",
    "        volume_cols.append(col)\n",
    "for col in volume_cols:\n",
    "    stocks.loc[:, col] = stocks[col].fillna(stocks[col].median())\n",
    "\n",
    "stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56035b11-0ba0-4084-9411-e1ec16ee0c72",
   "metadata": {},
   "source": [
    "Missing values in the dataset were handled by first removing weekends, as they are non-trading days. For Open, Close, High, and Low prices, missing values were filled using the average of the previous and next trading day's values, ensuring that data was only adjusted after the stock had begun trading. Volume data was completed using the median to maintain consistency and avoid skewing results. This approach ensures realistic stock data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ebe903-4657-413f-90d3-1e8a36d19206",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#check if duplicate rows exist\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of duplicate rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stocks\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stocks' is not defined"
     ]
    }
   ],
   "source": [
    "#check if duplicate rows exist\n",
    "print(\"Number of duplicate rows:\", stocks.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038224ea-b82f-4378-8b6e-276647f83b24",
   "metadata": {},
   "source": [
    "No duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b853c07-4bbd-4b21-8690-2466e05fbd74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stocks \u001b[38;5;241m=\u001b[39m stocks\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate_\u001b[39m\u001b[38;5;124m'\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stocks' is not defined"
     ]
    }
   ],
   "source": [
    "stocks = stocks.drop(['Unnamed: 0', 'Date_'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c9a54dd-328a-4830-85f5-c9152d6bdc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(ticker, start='2020-01-01', end='2024-10-01'):\n",
    "    data = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        data.columns = [col[0] for col in data.columns]\n",
    "\n",
    "    data['Return'] = data['Close'].pct_change()\n",
    "\n",
    "    data['Forward_Return'] = data['Return'].shift(-1)\n",
    "\n",
    "    data['Target'] = np.where(data['Forward_Return'] > 0.01, 2,  # Buy\n",
    "                             np.where(data['Forward_Return'] < -0.01, 0,  # Sell\n",
    "                                     1))  # Hold\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    print(f\"Class distribution: {data['Target'].value_counts()}\")\n",
    "    \n",
    "    return data[['Close', 'Volume', 'Return', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fc3fc41-40d0-42c7-8d39-36c6bc5facb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m google \u001b[38;5;241m=\u001b[39m fetch_stock_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36mfetch_stock_data\u001b[0;34m(ticker, start, end)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_stock_data\u001b[39m(ticker, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-10-01\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     data \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mdownload(ticker, start\u001b[38;5;241m=\u001b[39mstart, end\u001b[38;5;241m=\u001b[39mend)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns, pd\u001b[38;5;241m.\u001b[39mMultiIndex):\n\u001b[1;32m      5\u001b[0m         data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [col[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yf' is not defined"
     ]
    }
   ],
   "source": [
    "google = fetch_stock_data('GOOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eef19d68-95cb-44d5-aeec-9a4532dca594",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'google' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m google\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'google' is not defined"
     ]
    }
   ],
   "source": [
    "google.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23287e-ba67-4b0d-825d-4ad20d7baa33",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f237cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'google' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Logistic Regression Setup\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Define features (X) and target (y)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m X \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m google[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# ----- Additional Safeguard: Impute any remaining missing values in features -----\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# This ensures that even if some NaNs were missed during cleaning, they are filled.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'google' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ---------------------------\n",
    "# Logistic Regression Setup\n",
    "# ---------------------------\n",
    "\n",
    "# Create the target variable:\n",
    "# Predict if Close_AAPL increases (1) or decreases (0) the next day.\n",
    "#stocks[\"Target\"] = (stocks[\"Close_AAPL\"].shift(-1) > stocks[\"Close_AAPL\"]).astype(int)\n",
    "\n",
    "# Drop the last row (no \"next day\" available)\n",
    "#stocks = stocks[:-1]\n",
    "\n",
    "# Remove the Date column (non-numeric)\n",
    "#if \"Date\" in stocks.columns:\n",
    "#    stocks = stocks.drop(columns=[\"Date_\"])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = google.drop(columns=[\"Target\"])\n",
    "y = google[\"Target\"]\n",
    "\n",
    "# ----- Additional Safeguard: Impute any remaining missing values in features -----\n",
    "# This ensures that even if some NaNs were missed during cleaning, they are filled.\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Confirm no NaNs remain\n",
    "assert X.isnull().sum().sum() == 0, \"There are still missing values in the features!\"\n",
    "\n",
    "# ---------------------------\n",
    "# Split the Data\n",
    "# ---------------------------\n",
    "# Use a 70/15/15 split for training, validation, and testing.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Feature Scaling\n",
    "# ---------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# Train Logistic Regression Model\n",
    "# ---------------------------\n",
    "log_reg = LogisticRegression(multi_class = 'multinomial', max_iter=500, random_state=42, class_weight = 'balanced')\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Plot the ROC Curve\n",
    "# ---------------------------\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import cycle\n",
    "\n",
    "# Get predicted probabilities for the positive class on the test set.\n",
    "# Binarize y for ROC AUC score (e.g., one-vs-rest style)\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "y_score = log_reg.predict_proba(X_test_scaled)\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_bin, y_score, average='macro', multi_class='ovr')\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "macro_roc_auc = roc_auc_score(y_test_bin, y_score, average='macro', multi_class='ovr')\n",
    "print(f\"Multiclass ROC AUC (macro-average): {macro_roc_auc:.3f}\")\n",
    "\n",
    "colors = cycle(['blue', 'orange', 'green'])\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multiclass ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Plot the Logistic Regression Coefficients\n",
    "# ---------------------------\n",
    "# Retrieve the coefficients and corresponding feature names.\n",
    "coef = log_reg.coef_[0]\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame and sort by absolute coefficient values.\n",
    "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coef})\n",
    "coef_df['AbsCoefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='AbsCoefficient', ascending=False)\n",
    "\n",
    "# Plot the top 20 features.\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(coef_df['Feature'].head(20)[::-1], coef_df['Coefficient'].head(20)[::-1])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top 20 Logistic Regression Coefficients')\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Plot the Confusion Matrix\n",
    "# ---------------------------\n",
    "cm = confusion_matrix(y_test, log_reg.predict(X_test_scaled))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Logistic Regression Model')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Evaluate the Model\n",
    "# ---------------------------\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_val_pred = log_reg.predict(X_val_scaled)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(\"Validation Classification Report:\\n\", val_report)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_test_pred = log_reg.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Classification Report:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc34552-b12c-493e-b78c-72b41264a48e",
   "metadata": {},
   "source": [
    "### Neural Network Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150b58e-5ff6-4a55-89a6-1b10120fa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f48eb4-80e7-4de5-a15e-7cdeffe15e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multiclass_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')  # 3 output neurons for 3 classes with softmax activation\n",
    "    ])\n",
    "    \n",
    "    # Use categorical_crossentropy for multi-class classification\n",
    "    model.compile(optimizer='adam', \n",
    "                 loss='sparse_categorical_crossentropy',  # Use sparse if target is not one-hot encoded\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = build_multiclass_model(X_train.shape[1])\n",
    "\n",
    "# Calculate class weights\n",
    "weights = class_weight.compute_class_weight(class_weight='balanced', \n",
    "                                           classes=np.unique(y_train), \n",
    "                                           y=y_train)\n",
    "class_weights = dict(enumerate(weights))\n",
    "\n",
    "# Fit with class weights\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,  # Using integer labels directly\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be921a9-9622-4ed6-82c1-c9f88d9534ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Get the class with highest probability\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Sell', 'Hold', 'Buy']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Display prediction distribution\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "for value, count in zip(unique, counts):\n",
    "    class_name = {0: 'Sell', 1: 'Hold', 2: 'Buy'}[value]\n",
    "    print(f\"{class_name} (Class {value}): {count}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145335b-2cea-45ff-bae5-35a5690f51f7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35060ad5-8bab-4271-9a10-98e1ed4d84c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc355c-ce7d-436b-89cb-0ab25c31dff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb31714-8936-4453-aaf5-19970edbff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c558e-e1fb-4429-b9bc-06e8bbcce1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31799fab-55f5-4984-a623-be2b286c4e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2f8c8-831a-4832-811a-8c9759acae95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
