{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5030e3-8dee-48b7-8d68-98d4169f1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1535ce-e300-4eda-a6ca-784ba2a41b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stock data\n",
    "def fetch_stock_data(ticker, start='2020-01-01', end='2024-10-01'):\n",
    "    data = yf.download(ticker, start=start, end=end)\n",
    "\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        data.columns = [col[0] for col in data.columns]\n",
    "\n",
    "    data['Return'] = data['Close'].pct_change()\n",
    "\n",
    "    data['MA5'] = data['Close'].rolling(window=5).mean()\n",
    "    data['MA20'] = data['Close'].rolling(window=20).mean()\n",
    "    data['RSI'] = calculate_rsi(data['Close'], 14)\n",
    "\n",
    "    data['Forward_Return'] = data['Return'].shift(-1)\n",
    "\n",
    "    data['Target'] = np.where(data['Forward_Return'] > 0.01, 2,  # Buy\n",
    "                             np.where(data['Forward_Return'] < -0.01, 0,  # Sell\n",
    "                                     1))  # Hold\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    print(f\"Class distribution: {data['Target'].value_counts()}\")\n",
    "    \n",
    "    return data[['Close', 'Volume', 'Return', 'MA5', 'MA20', 'RSI', 'Target']]\n",
    "\n",
    "# Helper function for RSI calculation\n",
    "def calculate_rsi(close_prices, period=14):\n",
    "    delta = close_prices.diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "#Custom dataset class with normalization\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=30):\n",
    "        features = data.iloc[:, :-1].values\n",
    "        targets = data.iloc[:, -1].values\n",
    "        \n",
    "        # Normalize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.features = self.scaler.fit_transform(features)\n",
    "        self.targets = targets.astype(np.int64)\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx:idx + self.seq_length]\n",
    "        y = self.targets[idx + self.seq_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Improved positional encoding compatible with batch_first=True\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(1, max_len, d_model)  # Shape: [1, max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerStockPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=6, d_model=64, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(TransformerStockPredictor, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Create transformer with dropout for regularization\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Multiple classification heads for better learning\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, 3)  # 3 classes: Sell (0), Hold (1), Buy (2)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for better convergence\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        src = self.input_projection(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Apply attention mask to focus on more recent data\n",
    "        seq_len = src.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(src.device)\n",
    "        \n",
    "        output = self.transformer_encoder(src, mask=mask)\n",
    "        \n",
    "        # Use the last sequence element for prediction\n",
    "        output = self.classifier(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83dc13b5-1046-4d3d-ad79-4d8a35381dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Target\n",
      "1    572\n",
      "2    322\n",
      "0    280\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/50\n",
      "Train Loss: 1.1578, Train Acc: 34.86%\n",
      "Test Loss: 1.1067, Test Acc: 24.45%\n",
      "Prediction distribution: {np.int64(0): np.int64(200), np.int64(2): np.int64(29)}\n",
      "Epoch 2/50\n",
      "Train Loss: 1.1155, Train Acc: 37.38%\n",
      "Test Loss: 1.1058, Test Acc: 24.45%\n",
      "Epoch 3/50\n",
      "Train Loss: 1.1221, Train Acc: 37.81%\n",
      "Test Loss: 1.1067, Test Acc: 49.34%\n",
      "Epoch 4/50\n",
      "Train Loss: 1.1011, Train Acc: 36.94%\n",
      "Test Loss: 1.1255, Test Acc: 50.22%\n",
      "Epoch 5/50\n",
      "Train Loss: 1.0944, Train Acc: 39.56%\n",
      "Test Loss: 1.1014, Test Acc: 39.74%\n",
      "Epoch 6/50\n",
      "Train Loss: 1.1017, Train Acc: 37.81%\n",
      "Test Loss: 1.1022, Test Acc: 35.81%\n",
      "Prediction distribution: {np.int64(1): np.int64(51), np.int64(2): np.int64(178)}\n",
      "Epoch 7/50\n",
      "Train Loss: 1.0896, Train Acc: 42.62%\n",
      "Test Loss: 1.1116, Test Acc: 44.10%\n",
      "Epoch 8/50\n",
      "Train Loss: 1.0883, Train Acc: 42.30%\n",
      "Test Loss: 1.1012, Test Acc: 33.19%\n",
      "Epoch 9/50\n",
      "Train Loss: 1.0855, Train Acc: 40.98%\n",
      "Test Loss: 1.1005, Test Acc: 44.98%\n",
      "Early stopping triggered after 9 epochs\n",
      "Confusion Matrix:\n",
      "[[28 25  4]\n",
      " [37 72  2]\n",
      " [39 19  3]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Sell       0.27      0.49      0.35        57\n",
      "        Hold       0.62      0.65      0.63       111\n",
      "         Buy       0.33      0.05      0.09        61\n",
      "\n",
      "    accuracy                           0.45       229\n",
      "   macro avg       0.41      0.40      0.36       229\n",
      "weighted avg       0.46      0.45      0.42       229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data with more features\n",
    "ticker = 'GOOG'\n",
    "data = fetch_stock_data(ticker)\n",
    "dataset = StockDataset(data)\n",
    "\n",
    "# Split data with time-based approach (better for time series)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model setup with correct input dimension\n",
    "input_dim = data.shape[1] - 1\n",
    "model = TransformerStockPredictor(input_dim=input_dim)\n",
    "\n",
    "# Get class weights based on frequency\n",
    "class_counts = np.bincount(data['Target'].astype(int))\n",
    "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "class_weights = class_weights / class_weights.sum() \n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Use lower learning rate with weight decay for regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_test_acc = 0\n",
    "patience = 5\n",
    "counter = 0\n",
    "num_epochs = 50 \n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Add gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == y_batch).sum().item()\n",
    "        train_total += y_batch.size(0)\n",
    "    \n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    epoch_test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            outputs = model(x_batch)\n",
    "            \n",
    "            loss = criterion(outputs, y_batch)\n",
    "            epoch_test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_correct += (predicted == y_batch).sum().item()\n",
    "            test_total += y_batch.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    avg_test_loss = epoch_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    # Update learning rate based on validation loss\n",
    "    avg_test_loss = epoch_test_loss / len(test_loader) \n",
    "    scheduler.step(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {epoch_train_loss/len(train_loader):.4f}, Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Print prediction distribution\n",
    "    if epoch % 5 == 0:\n",
    "        unique, counts = np.unique(all_preds, return_counts=True)\n",
    "        print(f\"Prediction distribution: {dict(zip(unique, counts))}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if test_accuracy > best_test_acc:\n",
    "        best_test_acc = test_accuracy\n",
    "        torch.save(model.state_dict(), f'{ticker}_best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "# After training, get confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        outputs = model(x_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=['Sell', 'Hold', 'Buy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994b56e-de44-424d-bea8-3126924f3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Testing Accuracy', marker='x')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'{ticker} Stock Price Direction Prediction Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Also plot confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Sell', 'Hold', 'Buy'],\n",
    "            yticklabels=['Sell', 'Hold', 'Buy'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'{ticker} Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker = 'o')\n",
    "plt.plot(range(1, len(test_losses) + 1), test_losses, label='Validation Loss', marker = 'x')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'{ticker} Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{ticker}_learning_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b70d6-3dbf-4bb8-8588-c4f43ea9f125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c0b96-cc58-442f-93b5-1bc1ff37808f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251c253-32bd-4961-9a1d-8060e894861e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e12b6e-ccb1-4664-8885-64ed25c605c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
